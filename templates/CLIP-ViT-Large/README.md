# CLIP ViT-Large/14

OpenAI's powerful vision-language model for understanding images and text in a shared embedding space.

Unleash the power of zero-shot image classification with Nosana! Run CLIP on GPU-backed nodes for instant image recognition without any task-specific training.

## Key Features
- Vision Transformer (ViT) architecture with 428M parameters
- Zero-shot image classification and retrieval
- Joint image-text embedding space
- State-of-the-art performance on diverse benchmarks
- Robust to distribution shifts

## Configuration
- Port: 9000
- GPU: Required (4GB VRAM)
- REST API for image classification and embedding
- Supports both vision and text modalities
- Handles arbitrary image classification tasks
