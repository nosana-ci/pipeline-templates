# Qwen 2.5 Inference API

## Description
This template runs Qwen 2.5 (a powerful large language model) using vLLM and exposes it as an OpenAI-compatible API server on port 9000. It utilizes GPU resources for inference and supports up to 34,000 tokens in context.

Useful if you want to test or deploy Qwen 2.5 using OpenAI tools or clients.
